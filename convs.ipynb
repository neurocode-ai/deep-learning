{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "from leaf import Tensor\n",
    "from leaf.functions.function import Function\n",
    "from functools import partialmethod\n",
    "\n",
    "class Conv2d(Function):\n",
    "    # padding doesn't work correctly, should pad input not output...\n",
    "    def forward(self, x, w, stride=1, padding=0):\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        \n",
    "        batch_size, in_C_, in_H, in_W = x.shape\n",
    "        out_C, in_C, kernel_H, kernel_W = w.shape\n",
    "        stride_H, stride_W = stride\n",
    "        padding_H, padding_W = padding\n",
    "        \n",
    "        assert in_C == in_C_\n",
    "        \n",
    "        out_H = ((in_H + 2 * padding_H - (kernel_H - 1) - 1) // stride_H) + 1\n",
    "        out_W = ((in_W + 2 * padding_W - (kernel_W - 1) - 1) // stride_W) + 1\n",
    "        \n",
    "        self.save_for_backward(x, w, stride)\n",
    "        tw = w.reshape(out_C, -1).T\n",
    "        result = np.zeros((batch_size, out_C, out_H, out_W)).astype(x.dtype)\n",
    "        for h in range(out_H):\n",
    "            for w in range(out_W):\n",
    "                ih, iw = h * stride_H, w * stride_W\n",
    "                result[:, :, h, w] = np.dot(\n",
    "                    x[:, :, ih:ih+kernel_H, iw:iw+kernel_W].reshape(batch_size, -1), tw)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad, **kwargs):\n",
    "        x, w, stride, = self.saved_tensors\n",
    "        _, _, out_H, out_W = grad.shape\n",
    "        batch_size, in_C_, in_H, in_W = x.shape\n",
    "        out_C, in_C, kernel_H, kernel_W = w.shape\n",
    "        stride_H, stride_W = stride\n",
    "        \n",
    "        dx = np.zeros((batch_size, in_C_, in_H, in_W)).astype(x.dtype)\n",
    "        dw = np.zeros((out_C, in_C, kernel_H, kernel_W)).astype(w.dtype)\n",
    "        tw = w.reshape(out_C, -1)\n",
    "        for h in range(out_H):\n",
    "            for w in range(out_W):\n",
    "                ih, iw = h * stride_H, w * stride_W\n",
    "                g = grad[:, :, h, w]\n",
    "                dw += g.T.dot(\n",
    "                    x[:, :, ih:ih+kernel_H, iw:iw+kernel_W].reshape(batch_size, -1)).reshape(dw.shape)\n",
    "                dx[:, :, ih:ih+kernel_H, iw:iw+kernel_W] += g.dot(\n",
    "                    tw).reshape(batch_size, in_C_, kernel_H, kernel_W)\n",
    "        \n",
    "        return  dx, dw\n",
    "    \n",
    "\n",
    "class MaxPool2d(Function):\n",
    "    def forward(self, x, kernel_size=2, stride=1, padding=0):\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        \n",
    "        batch_size, in_C, in_H, in_W = x.shape\n",
    "        kernel_H, kernel_W = kernel_size\n",
    "        stride_H, stride_W = stride\n",
    "        padding_H, padding_W = padding\n",
    "        \n",
    "        out_H = ((in_H + 2 * padding_H - (kernel_H - 1) - 1) // stride_H) + 1\n",
    "        out_W = ((in_W + 2 * padding_W - (kernel_W - 1) - 1) // stride_W) + 1\n",
    "        \n",
    "        self.save_for_backward(x, kernel_size, stride)\n",
    "        result = np.zeros((batch_size, in_C, out_H, out_W)).astype(x.dtype)\n",
    "        for h in range(out_H):\n",
    "            for w in range(out_W):\n",
    "                ih, iw = h * stride_H, w * stride_W\n",
    "                result[:, :, h, w] = np.max(\n",
    "                    x[:, :, ih:ih+kernel_H, iw:iw+kernel_W].reshape(batch_size, in_C, -1), axis=-1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad, **kwargs):\n",
    "        x, kernel_size, stride, = self.saved_tensors\n",
    "        _, _, out_H, out_W = grad.shape\n",
    "        batch_size, in_C, in_H, in_W = x.shape\n",
    "        kernel_H, kernel_W = kernel_size\n",
    "        stride_H, stride_W = stride\n",
    "        \n",
    "        dx = np.zeros((batch_size, in_C, in_H, in_W)).astype(x.dtype)\n",
    "        for h in range(out_H):\n",
    "            for w in range(out_W):\n",
    "                ih, iw = h * stride_H, w * stride_W\n",
    "                xx = x[:, :, ih:ih+kernel_H, iw:iw+kernel_W].reshape(batch_size, in_C, -1)\n",
    "                mask = (xx.max(axis=-1, keepdims=True) == xx)\n",
    "                xd = (xx * mask)\n",
    "                gg = np.expand_dims(grad[:, :, h, w], axis=-1)\n",
    "                dx[:, :, ih:ih+kernel_H, iw:iw+kernel_W] = (gg * xd).reshape(batch_size, in_C, kernel_H, kernel_W)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "setattr(Tensor, 'conv2d', partialmethod(Conv2d.apply, Conv2d))\n",
    "setattr(Tensor, 'maxpool2d', partialmethod(MaxPool2d.apply, MaxPool2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 4, 4)\n",
      "torch.Size([8, 4, 4, 4])\n",
      "forward pass with convolution passed test\n",
      "backward pass with convolution passed test\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = Tensor.uniform(8, 2, 12, 12, requires_grad=True)\n",
    "w = Tensor.uniform(4, 2, 5, 5, requires_grad=True)\n",
    "\n",
    "tx = torch.tensor(x.data, requires_grad=True)\n",
    "tw = torch.tensor(w.data, requires_grad=True)\n",
    "\n",
    "lco = x.conv2d(w, stride=2, padding=0)\n",
    "print(lco.shape)\n",
    "\n",
    "tco = torch.nn.functional.conv2d(tx, tw, stride=2, groups=1, padding=0)\n",
    "print(tco.shape)\n",
    "\n",
    "np.testing.assert_allclose(tco.detach().numpy(), lco.data, rtol=1e-6, atol=1e-3)\n",
    "print('forward pass with convolution passed test')\n",
    "\n",
    "lco.mean().backward()\n",
    "tco.mean().backward()\n",
    "\n",
    "np.testing.assert_allclose(tw.grad.numpy(), w.grad, rtol=1e-6, atol=1e-3)\n",
    "np.testing.assert_allclose(tx.grad.numpy(), x.grad, rtol=1e-6, atol=1e-3)\n",
    "print('backward pass with convolution passed test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2, 11, 11)\n",
      "torch.Size([8, 2, 11, 11])\n",
      "forward pass with maxpool2d passed test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wille\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-06, atol=0.001\n\nMismatched elements: 569 / 2304 (24.7%)\nMax absolute difference: 0.00206621\nMax relative difference: 22299.613\n x: array([[[[0.000517, 0.000517, 0.      , ..., 0.      , 0.      ,\n          0.      ],\n         [0.      , 0.      , 0.      , ..., 0.00155 , 0.000517,...\n y: array([[[[ 6.796276e-06,  5.115369e-07, -0.000000e+00, ...,\n          -0.000000e+00, -0.000000e+00, -0.000000e+00],\n         [-0.000000e+00, -0.000000e+00, -0.000000e+00, ...,...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fc28ea343ff1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'backward pass with maxpool2d passed test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    838\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-06, atol=0.001\n\nMismatched elements: 569 / 2304 (24.7%)\nMax absolute difference: 0.00206621\nMax relative difference: 22299.613\n x: array([[[[0.000517, 0.000517, 0.      , ..., 0.      , 0.      ,\n          0.      ],\n         [0.      , 0.      , 0.      , ..., 0.00155 , 0.000517,...\n y: array([[[[ 6.796276e-06,  5.115369e-07, -0.000000e+00, ...,\n          -0.000000e+00, -0.000000e+00, -0.000000e+00],\n         [-0.000000e+00, -0.000000e+00, -0.000000e+00, ...,..."
     ]
    }
   ],
   "source": [
    "mx = Tensor.uniform(8, 2, 12, 12, requires_grad=True)\n",
    "\n",
    "lmo = mx.maxpool2d(kernel_size=2, stride=1, padding=0)\n",
    "print(lmo.shape)\n",
    "\n",
    "mtx = torch.tensor(mx.data, requires_grad=True)\n",
    "\n",
    "tmo = torch.nn.functional.max_pool2d(mtx, (2, 2), stride=1, padding=0)\n",
    "print(tmo.shape)\n",
    "\n",
    "np.testing.assert_allclose(tmo.detach().numpy(), lmo.data, rtol=1e-6, atol=1e-3)\n",
    "print('forward pass with maxpool2d passed test')\n",
    "\n",
    "lmo.mean().backward()\n",
    "tmo.mean().backward()\n",
    "np.testing.assert_allclose(mtx.grad.numpy(), mx.grad, rtol=1e-6, atol=1e-3)\n",
    "print('backward pass with maxpool2d passed test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
